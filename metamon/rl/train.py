import os
from functools import partial
from typing import List, Optional

import wandb

import amago

from metamon.env import get_metamon_teams
from metamon.interface import (
    TokenizedObservationSpace,
    ActionSpace,
    RewardFunction,
)
from metamon.tokenizer import get_tokenizer
from metamon.data import ParsedReplayDataset
from metamon.rl.metamon_to_amago import (
    MetamonAMAGOExperiment,
    MetamonAMAGODataset,
    make_baseline_env,
    make_placeholder_env,
)
from metamon import baselines


WANDB_PROJECT = os.environ.get("METAMON_WANDB_PROJECT")
WANDB_ENTITY = os.environ.get("METAMON_WANDB_ENTITY")
EVAL_OPPONENTS = [
    baselines.heuristic.basic.PokeEnvHeuristic,
    baselines.heuristic.basic.Gen1BossAI,
    baselines.heuristic.basic.Grunt,
    baselines.heuristic.basic.GymLeader,
    baselines.heuristic.kaizo.EmeraldKaizo,
]


def add_cli(parser):
    parser.add_argument(
        "--run_name",
        required=True,
        help="Give the run a name to identify logs and checkpoints.",
    )
    parser.add_argument("--obs_space", type=str, default="TeamPreviewObservationSpace")
    parser.add_argument("--reward_function", type=str, default="DefaultShapedReward")
    parser.add_argument("--action_space", type=str, default="DefaultActionSpace")
    parser.add_argument(
        "--ckpt_dir",
        type=str,
        required=True,
        help="Path to save checkpoints. Find checkpoints under {ckpt_dir}/{run_name}/ckpts/",
    )
    parser.add_argument(
        "--ckpt",
        type=int,
        default=None,
        help="Resume training from an existing run with this run_name. Provide the epoch checkpoint to load.",
    )
    parser.add_argument(
        "--finetune_from_path",
        type=str,
        default=None,
        help="Path to a checkpoint (from another run) to initialize weights.",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=100,
        help="Number of epochs to train for. In offline RL model, an epoch is an arbitrary interval (here: 25k) of training steps on a fixed dataset.",
    )
    parser.add_argument(
        "--batch_size_per_gpu",
        type=int,
        default=12,
        help="Batch size per GPU. Total batch size is batch_size_per_gpu * num_gpus.",
    )
    parser.add_argument(
        "--grad_accum",
        type=int,
        default=1,
        help="Number of gradient accumulations per update.",
    )
    parser.add_argument(
        "--model_gin_config",
        type=str,
        required=True,
        help="Path to a gin config file that edits the model architecture. See provided rl/configs/models/",
    )
    parser.add_argument(
        "--train_gin_config",
        type=str,
        required=True,
        help="Path to a gin config file that edits the training or hparams. See provided rl/configs/training/",
    )
    parser.add_argument(
        "--tokenizer",
        type=str,
        default="DefaultObservationSpace-v1",
        help="The tokenizer to use for the text observation space. See metamon.tokenizer for options.",
    )
    parser.add_argument(
        "--dloader_workers",
        type=int,
        default=10,
        help="Number of workers for the data loader.",
    )
    parser.add_argument(
        "--parsed_replay_dir",
        type=str,
        default=None,
        help="Path to the parsed replay directory. Defaults to the official huggingface version.",
    )
    parser.add_argument(
        "--custom_replay_dir",
        type=str,
        default=None,
        help="Path to an optional second parsed replay dataset (e.g., self-play data you've collected).",
    )
    parser.add_argument(
        "--custom_replay_sample_weight",
        type=float,
        default=0.25,
        help="[0, 1] portion of each batch to sample from the custom dataset (if provided).",
    )
    parser.add_argument(
        "--async_env_mp_context",
        type=str,
        default="spawn",
        help="Async environment setup method. Try 'forkserver' or 'fork' if using multiple GPUs or if you run into issues.",
    )
    parser.add_argument(
        "--eval_gens",
        type=int,
        nargs="+",
        default=[1, 2, 3, 4, 9],
        help="Generations (of OU) to play against heuristics between training epochs. Win rates usually saturate at 90%+ quickly, so this is mostly a sanity-check. Reduce gens to save time on launch!",
    )
    parser.add_argument("--log", action="store_true", help="Log to wandb.")
    return parser


def create_offline_dataset(
    obs_space: TokenizedObservationSpace,
    action_space: ActionSpace,
    reward_function: RewardFunction,
    parsed_replay_dir: str,
    custom_replay_dir: Optional[str] = None,
    custom_replay_sample_weight: float = 0.25,
):
    dset_kwargs = {
        "observation_space": obs_space,
        "action_space": action_space,
        "reward_function": reward_function,
        # amago will handle sequence lengths on its side
        "max_seq_len": None,
        "verbose": True,  # False to hide dset setup progress bar
    }
    parsed_replays_amago = MetamonAMAGODataset(
        dset_name="Metamon Parsed Replays",
        parsed_replay_dset=ParsedReplayDataset(
            dset_root=parsed_replay_dir, **dset_kwargs
        ),
    )
    if custom_replay_dir is not None:
        custom_dset_amago = MetamonAMAGODataset(
            dset_name="Custom Parsed Replays",
            parsed_replay_dset=ParsedReplayDataset(
                dset_root=custom_replay_dir, **dset_kwargs
            ),
        )
        amago_dataset = amago.loading.MixtureOfDatasets(
            datasets=[parsed_replays_amago, custom_dset_amago],
            sampling_weights=[
                1 - custom_replay_sample_weight,
                custom_replay_sample_weight,
            ],
        )
    else:
        amago_dataset = parsed_replays_amago
    return amago_dataset


def create_offline_rl_trainer(
    ckpt_dir: str,
    run_name: str,
    model_gin_config: str,
    train_gin_config: str,
    obs_space: TokenizedObservationSpace,
    action_space: ActionSpace,
    reward_function: RewardFunction,
    amago_dataset: amago.loading.Dataset,
    eval_gens: List[int] = [1, 2, 3, 4, 9],
    async_env_mp_context: str = "spawn",
    dloader_workers: int = 8,
    epochs: int = 40,
    grad_accum: int = 1,
    batch_size_per_gpu: int = 16,
    log: bool = False,
    wandb_project: str = WANDB_PROJECT,
    wandb_entity: str = WANDB_ENTITY,
):
    # configuration
    config = {"MetamonTstepEncoder.tokenizer": obs_space.tokenizer}
    amago.cli_utils.use_config(config, [model_gin_config, train_gin_config])

    # validation environments (evaluated throughout training)
    make_envs = [
        partial(
            make_baseline_env,
            battle_format=f"gen{gen}ou",
            observation_space=obs_space,
            action_space=action_space,
            reward_function=reward_function,
            player_team_set=get_metamon_teams(f"gen{gen}ou", "competitive"),
            opponent_type=opponent,
        )
        for gen in set(eval_gens)
        for opponent in EVAL_OPPONENTS
    ]
    experiment = MetamonAMAGOExperiment(
        ## required ##
        run_name=run_name,
        ckpt_base_dir=ckpt_dir,
        # max_seq_len = should be set in the gin file
        dataset=amago_dataset,
        # tstep_encoder_type = should be set in the gin file
        # traj_encoder_type = should be set in the gin file
        # agent_type = should be set in the gin file
        val_timesteps_per_epoch=300,  # per actor
        ## environment ##
        make_train_env=partial(make_placeholder_env, obs_space, action_space),
        make_val_env=make_envs,
        env_mode="async",
        async_env_mp_context=async_env_mp_context,
        parallel_actors=len(make_envs),
        # no exploration
        exploration_wrapper_type=None,
        sample_actions=True,
        force_reset_train_envs_every=None,
        ## logging ##
        log_to_wandb=log,
        wandb_project=wandb_project,
        wandb_entity=wandb_entity,
        verbose=True,
        log_interval=300,
        ## replay ##
        padded_sampling="none",
        dloader_workers=dloader_workers,
        ## learning schedule ##
        epochs=epochs,
        # entirely offline RL
        start_learning_at_epoch=0,
        start_collecting_at_epoch=float("inf"),
        train_timesteps_per_epoch=0,
        train_batches_per_epoch=25_000 * grad_accum,
        val_interval=1,
        ckpt_interval=2,
        ## optimization ##
        batch_size=batch_size_per_gpu,
        batches_per_update=grad_accum,
        mixed_precision="no",
    )
    return experiment


if __name__ == "__main__":
    from argparse import ArgumentParser
    from metamon.interface import (
        ALL_OBSERVATION_SPACES,
        ALL_REWARD_FUNCTIONS,
        ALL_ACTION_SPACES,
    )

    parser = ArgumentParser()
    add_cli(parser)
    args = parser.parse_args()

    # metamon dataset
    obs_space = TokenizedObservationSpace(
        ALL_OBSERVATION_SPACES[args.obs_space](), get_tokenizer(args.tokenizer)
    )
    reward_function = ALL_REWARD_FUNCTIONS[args.reward_function]()
    action_space = ALL_ACTION_SPACES[args.action_space]()

    amago_dataset = create_offline_dataset(
        obs_space=obs_space,
        action_space=action_space,
        reward_function=reward_function,
        parsed_replay_dir=args.parsed_replay_dir,
        custom_replay_dir=args.custom_replay_dir,
        custom_replay_sample_weight=args.custom_replay_sample_weight,
    )
    experiment = create_offline_rl_trainer(
        ckpt_dir=args.ckpt_dir,
        run_name=args.run_name,
        model_gin_config=args.model_gin_config,
        train_gin_config=args.train_gin_config,
        obs_space=obs_space,
        action_space=action_space,
        reward_function=reward_function,
        amago_dataset=amago_dataset,
        eval_gens=args.eval_gens,
        async_env_mp_context=args.async_env_mp_context,
        dloader_workers=args.dloader_workers,
        epochs=args.epochs,
        grad_accum=args.grad_accum,
        batch_size_per_gpu=args.batch_size_per_gpu,
        log=args.log,
        wandb_project=WANDB_PROJECT,
        wandb_entity=WANDB_ENTITY,
    )
    experiment.start()
    if args.ckpt is not None:
        assert (
            args.finetune_from_path is None
        ), "Provide --ckpt or --finetune_from_path, not both"
        experiment.load_checkpoint(args.ckpt)
    elif args.finetune_from_path is not None:
        experiment.load_checkpoint_from_path(
            args.finetune_from_path,
            is_accelerate_state=not ".pt" in args.finetune_from_path,
        )
    experiment.learn()
    wandb.finish()

import math
from abc import ABC, abstractmethod
from typing import Callable, Type

import torch
from torch import nn
from torch.nn import functional as F
from einops import rearrange, repeat
import gin

from metamon.data.tokenizer import PokemonTokenizer


class MetamonILModel(nn.Module, ABC):
    """
    If you want to build a custom model for replay data:
        1. Inherit from this class
        2. Define the forward pass with `inner_forward`
        3. You should be able to use it with our training pipeline and online baseline system w/o any changes.
    """

    def __init__(
        self, tokenizer: PokemonTokenizer, numerical_features: int, num_actions: int
    ):
        super().__init__()
        self.tokenizer = tokenizer
        self.num_actions = num_actions
        self.numerical_features = numerical_features

    def forward(self, token_inputs, numerical_inputs, hidden_state=None):
        return self.inner_forward(
            token_inputs, numerical_inputs, hidden_state=hidden_state
        )

    @abstractmethod
    def inner_forward(sef, token_inputs, numerical_inputs, hidden_state=None):
        pass


@gin.configurable
class TokenEmbedding(nn.Module):
    """
    Map integer tokens to a sequence of vector representations.

    Just an `nn.Embedding` with a +1 shift to account for the "unknown"
    (-1) token. Separated out like this to make it easy to use
    this layer as an initialization for online RL policies.
    """

    def __init__(self, tokenizer: PokemonTokenizer, emb_dim: int = 32):
        super().__init__()
        self.tokenizer = tokenizer
        self.vocab_size = len(tokenizer) + 1
        self.text_emb = nn.Embedding(self.vocab_size, embedding_dim=emb_dim)
        self.output_dim = emb_dim

    def forward(self, tokens: torch.Tensor) -> torch.Tensor:
        shifted_tokens = (tokens + 1).clamp(0, self.vocab_size)
        text_emb = self.text_emb(shifted_tokens)
        return text_emb


@gin.configurable
class MultiModalEmbedding(nn.Module):
    """
    Take the text embedding and add on a representation of the numerical
    part of our observation, creating a multimodal sequence.

    We do not need embeddings to indicate which timestep of this new sequence
    is from text vs. numerical because they are padded to be a consistent length.
    """

    def __init__(
        self,
        token_emb_dim: int,
        numerical_d_inp: int,
        output_dim: int,
        numerical_tokens: int = 6,
        dropout: float = 0.05,
    ):
        super().__init__()
        self.text_emb = nn.Linear(token_emb_dim, output_dim)
        self.num_emb = nn.Linear(numerical_d_inp, numerical_tokens * output_dim)
        self.numerical_tokens = numerical_tokens
        self.dropout = nn.Dropout(dropout)
        self.output_dim = output_dim

    def forward(
        self, text_emb: torch.Tensor, numerical_features: torch.Tensor
    ) -> torch.Tensor:
        text_emb = F.leaky_relu(self.dropout(self.text_emb(text_emb)))
        num_emb = F.leaky_relu(self.dropout(self.num_emb(numerical_features)))
        num_emb = rearrange(num_emb, "b l (l2 d) -> b l l2 d", l2=self.numerical_tokens)
        seq = torch.cat((text_emb, num_emb), dim=-2)
        return seq


class FixedPosEmb(nn.Module):
    def __init__(self, d_model: int):
        super().__init__()
        self.d_model = d_model

    def forward(self, pos_idxs: torch.LongTensor):
        B, L = pos_idxs.shape
        emb = torch.zeros(
            (B, L, self.d_model), device=pos_idxs.device, dtype=torch.float32
        )
        coeff = torch.exp(
            (
                torch.arange(0, self.d_model, 2, device=emb.device, dtype=torch.float32)
                * -(math.log(10000.0) / self.d_model)
            )
        )
        emb[..., 0::2] = torch.sin(pos_idxs.float().unsqueeze(-1) * coeff)
        emb[..., 1::2] = torch.cos(pos_idxs.float().unsqueeze(-1) * coeff)
        return emb


@gin.configurable
class TimestepTransformer(nn.Module):
    """
    Take the multimodal sequence, add on a few blank ("scratch") tokens
    like it's a Vision Transformer, pass the new seq through a small Transformer,
    then treat the scratch tokens as the output.
    """

    def __init__(
        self,
        scratch_tokens: int = 2,
        d_model: int = 256,
        n_heads: int = 8,
        n_layers: int = 3,
        dropout: float = 0.05,
    ):
        super().__init__()
        self.scratch_tokens = scratch_tokens
        self.d_model = d_model
        self.pos_emb = FixedPosEmb(d_model=d_model)
        emb_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_model * 4,
            activation=F.leaky_relu,
            dropout=dropout,
            batch_first=True,
            norm_first=True,
        )
        self.emb_transformer = nn.TransformerEncoder(
            emb_layer, num_layers=n_layers, norm=nn.LayerNorm(d_model)
        )
        self.output_dim = scratch_tokens * d_model

    def forward(self, emb: torch.Tensor) -> torch.Tensor:
        # create scratch token sequence
        B, Global_L, Local_L, D_emb = emb.shape
        assert self.d_model == D_emb
        device = emb.device
        scratch_tokens = torch.zeros(
            B, Global_L, self.scratch_tokens, D_emb, device=device
        )
        seq = torch.cat((emb, scratch_tokens), dim=-2)
        # this sequence dimension is really the length of each turn,
        # so you fold the length of the battle into the batch dimension
        seq = rearrange(seq, "b l l2 d -> (b l) l2 d")
        B, L, _ = seq.shape
        pos_idxs = torch.arange(0, L, device=seq.device).long().unsqueeze(0)
        # add positional embeddings manually
        pos_embs = self.pos_emb(pos_idxs)
        emb = self.emb_transformer(seq + pos_embs)
        # keep only scratch tokens, undoing the previous length fold
        emb = emb[:, -self.scratch_tokens :, :]
        emb = rearrange(
            emb, "(b l) l2 d -> b l (l2 d)", l=Global_L, l2=self.scratch_tokens
        )
        return emb


class TurnEmbedding(nn.Module, ABC):
    """
    Map the multimodal (text, numerical) features of each turn in a battle to a
    fixed-size representation.

    Separated into an abstract class to make it more organized to pull pretrained
    versions into online/offline RL policies.
    """

    def __init__(
        self,
        tokenizer: PokemonTokenizer,
        token_embedding_dim: int,
        numerical_features: int,
    ):
        super().__init__()
        self.token_embedding = TokenEmbedding(
            tokenizer=tokenizer, emb_dim=token_embedding_dim
        )
        self.numerical_features = numerical_features
        self.tokenizer = tokenizer

    @property
    @abstractmethod
    def output_dim(self):
        pass

    @abstractmethod
    def forward(
        self, token_inputs: torch.Tensor, numerical_inputs: torch.Tensor
    ) -> torch.Tensor:
        pass


@gin.configurable
class TransformerTurnEmbedding(TurnEmbedding):
    def __init__(
        self,
        tokenizer: PokemonTokenizer,
        token_embedding_dim: int,
        numerical_features: int,
        scratch_tokens: int = 4,
        d_model: int = 100,
        n_heads: int = 5,
        n_layers: int = 3,
        dropout: float = 0.05,
    ):
        super().__init__(
            tokenizer,
            token_embedding_dim=token_embedding_dim,
            numerical_features=numerical_features,
        )
        self.tformer_embedding = TimestepTransformer(
            scratch_tokens=scratch_tokens,
            d_model=d_model,
            n_heads=n_heads,
            n_layers=n_layers,
            dropout=dropout,
        )
        self.multimodal_fuse = MultiModalEmbedding(
            token_emb_dim=self.token_embedding.output_dim,
            numerical_d_inp=self.numerical_features,
            output_dim=self.tformer_embedding.d_model,
        )

    @property
    def output_dim(self):
        return self.tformer_embedding.output_dim

    def forward(self, token_inputs, numerical_inputs):
        token_emb = self.token_embedding(token_inputs)
        tstep_seq = self.multimodal_fuse(token_emb, numerical_features=numerical_inputs)
        tstep_emb = self.tformer_embedding(tstep_seq)
        return tstep_emb


@gin.configurable
class FFTurnEmbedding(TurnEmbedding):
    def __init__(
        self,
        tokenizer: PokemonTokenizer,
        token_embedding_dim: int,
        numerical_features: int,
        d_hidden: int = 2048,
        d_emb: int = 400,
        dropout: float = 0.05,
    ):
        super().__init__(
            tokenizer,
            token_embedding_dim=token_embedding_dim,
            numerical_features=numerical_features,
        )
        self.d_emb = d_emb
        inp_dim = self.token_embedding.output_dim * 87 + self.numerical_features
        self.merge1 = nn.Linear(inp_dim, d_hidden)
        self.merge2 = nn.Linear(d_hidden, d_hidden)
        self.merge3 = nn.Linear(d_hidden, d_emb)
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(d_emb)

    @property
    def output_dim(self):
        return self.d_emb

    def forward(self, token_inputs, numerical_inputs):
        token_emb = self.token_embedding(token_inputs)
        token_emb = rearrange(token_emb, "b l1 l2 d -> b l1 (l2 d)")
        inp = torch.cat((token_emb, numerical_inputs), dim=-1)
        emb = F.leaky_relu(self.dropout(self.merge1(inp)))
        emb = F.leaky_relu(self.dropout(self.merge2(emb)))
        emb = self.norm(self.merge3(emb))
        return emb


@gin.configurable
class GRUModel(MetamonILModel):
    """
    Use a basic GRU to process sequences of Turn representations to produce
    action logits.
    """

    def __init__(
        self,
        tokenizer: PokemonTokenizer,
        numerical_features: int,
        token_embedding_dim: int = 100,
        num_actions: int = 9,
        turn_embedding_Cls: Type[TurnEmbedding] = TransformerTurnEmbedding,
        rnn_d_hidden: int = 512,
        rnn_layers: int = 2,
        rnn_dropout: float = 0.05,
    ):
        super().__init__(
            tokenizer=tokenizer,
            numerical_features=numerical_features,
            num_actions=num_actions,
        )
        self.turn_embedding = turn_embedding_Cls(
            tokenizer=tokenizer,
            numerical_features=numerical_features,
            token_embedding_dim=token_embedding_dim,
        )
        self.rnn = nn.GRU(
            input_size=self.turn_embedding.output_dim,
            hidden_size=rnn_d_hidden,
            num_layers=rnn_layers,
            dropout=rnn_dropout,
            batch_first=True,
        )
        self.out = nn.Linear(rnn_d_hidden, num_actions)

    @torch.compile
    def inner_forward(self, token_inputs, numerical_inputs, hidden_state=None):
        turn_emb = self.turn_embedding(
            token_inputs=token_inputs, numerical_inputs=numerical_inputs
        )
        rnn_out, new_hidden_state = self.rnn(turn_emb, hidden_state)
        out = self.out(rnn_out)
        return out, new_hidden_state
